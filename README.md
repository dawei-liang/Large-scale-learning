# Spark-Large-scale-learning

Install pyspark on Win10(Version Spark 2.1.0 + Hadoop 2.6.4):

1) All necessary packages:

http://blog.csdn.net/yaoqiwaimai/article/details/59114881   follow the steps to setup or unzip all packages (No need to root as in the last instruction)

2) Setups:

https://www.zhihu.com/question/35973656   follow the steps to check the paths of all environment variables

To change environment variables: Right click 'My computer' -> 'Advanced system settings' -> 'Advanced' -> 'Environment Variables'

3) Tested and passed on Jupyter notebook 2.7 
